关键字：序列池化，长短期记忆网络

关键字：序列池化，长短期记忆网络

问题描述：使用一个长短期记忆网络训练IMDB数据集时，出现输入形状错误，错误提示：输入(X)和输入(标签)应具有相同的形状。

问题描述：使用一个长短期记忆网络训练IMDB数据集时，出现输入形状错误，错误提示：输入(X)和输入(标签)应具有相同的形状。

报错信息：

报错信息：

<ipython-input-7-fd22a596e844> in train(use_cuda, train_program, params_dirname)
     41         event_handler=event_handler,
     42         reader=train_reader,
---> 43         feed_order=feed_order)

/opt/conda/envs/py35-paddle1.0.0/lib/python3.5/site-packages/paddle/fluid/contrib/trainer.py in train(self, num_epochs, event_handler, reader, feed_order)
    403         else:
    404             self._train_by_executor(num_epochs, event_handler, reader,
--> 405                                     feed_order)
    406 
    407     def test(self, reader, feed_order):

/opt/conda/envs/py35-paddle1.0.0/lib/python3.5/site-packages/paddle/fluid/contrib/trainer.py in _train_by_executor(self, num_epochs, event_handler, reader, feed_order)
    481             exe = executor.Executor(self.place)
    482             reader = feeder.decorate_reader(reader, multi_devices=False)
--> 483             self._train_by_any_executor(event_handler, exe, num_epochs, reader)
    484 
    485     def _train_by_any_executor(self, event_handler, exe, num_epochs, reader):

/opt/conda/envs/py35-paddle1.0.0/lib/python3.5/site-packages/paddle/fluid/contrib/trainer.py in _train_by_any_executor(self, event_handler, exe, num_epochs, reader)
    510                                       fetch_list=[
    511                                           var.name
--> 512                                           for var in self.train_func_outputs
    513                                       ])
    514                 else:

/opt/conda/envs/py35-paddle1.0.0/lib/python3.5/site-packages/paddle/fluid/executor.py in run(self, program, feed, fetch_list, feed_var_name, fetch_var_name, scope, return_numpy, use_program_cache)
    468 
    469         self._feed_data(program, feed, feed_var_name, scope)
--> 470         self.executor.run(program.desc, scope, 0, True, True)
    471         outs = self._fetch_data(fetch_list, fetch_var_name, scope)
    472         if return_numpy:

EnforceNotMet: Enforce failed. Expected framework::slice_ddim(x_dims, 0, rank - 1) == framework::slice_ddim(label_dims, 0, rank - 1), but received framework::slice_ddim(x_dims, 0, rank - 1):31673 != framework::slice_ddim(label_dims, 0, rank - 1):128.
Input(X) and Input(Label) shall have the same shape except the last dimension. at [/paddle/paddle/fluid/operators/cross_entropy_op.cc:37]
PaddlePaddle Call Stacks:

问题复现：在构建一个长短期记忆网络时，首先使用fluid.layers.fc定义了一个全连接层，然后又使用fluid.layers.dynamic_lstm创建了一个长短期记忆单元，最后使用使用这个两个进行分类输出，结果就会出现上面的错误，错误代码如下：

emb = fluid.layers.embedding(input=data, size=[input_dim, emb_dim], is_sparse=True)
fc1 = fluid.layers.fc(input=emb, size=hid_dim)
lstm1, cell1 = fluid.layers.dynamic_lstm(input=fc1, size=hid_dim)
prediction = fluid.layers.fc(input=[fc1, lstm1], size=class_dim, act='softmax')

解决问题：搭建一个长短期记忆网络时，在执行最好一层分类器前还要经过一个序列进行池化的接口，将上面的全连接层和长短期记忆单元的输出全部时间步的特征进行池化，最后才执行分类器输出。正确代码如下：

emb = fluid.layers.embedding(input=data, size=[input_dim, emb_dim], is_sparse=True)
fc1 = fluid.layers.fc(input=emb, size=hid_dim)
lstm1, cell1 = fluid.layers.dynamic_lstm(input=fc1, size=hid_dim)
fc_last = fluid.layers.sequence_pool(input=fc1, pool_type='max')
lstm_last = fluid.layers.sequence_pool(input=lstm1, pool_type='max')
prediction = fluid.layers.fc(input=[fc_last, lstm_last], size=class_dim, act='softmax')

本文分享 CSDN - 飞桨PaddlePaddle。
如有侵权，请联系 support@oschina.cn 删除。
本文参与“OSC源创计划”，欢迎正在阅读的你也加入，一起分享。

