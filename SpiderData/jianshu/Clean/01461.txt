
摘要


用python写爬虫的时候没我们经常遇到https认证的网站，采用常用模块requests模块，我们一般在请求中将verify设置成假，免证书验证，但是这些都是理想状态，https请求很容易报错，一旦报错就难以解决。


举个列子

编写一个简单的列子，我们的目标对象是一个https的网站，它的编码是gb2312，按照常用手法，我们设置免验证的方法，按照我们正常的逻辑，肯定是能成功，但是这里却不符合逻辑

下面开始运行代码：

报错的信息，无非是ssl的各种报错，我这里是 Caused by SSLError(SSLError("bad handshake: SysCallError(-1, 'Unexpected EOF')",),)，然今天今天要讲的就是解决一切SSLError


解决思路

既然是http是认证出了问题，那么我们就去修改代码解决，代码解决不了，我就让第三方程序去解决，那这第三方软件都属于那些？很多，就是抓包软件并且支持https，比如像fiddler什么的软件...这种法法的原理：就是一个代理服务器原理，python写的爬虫会报错，但代理服务器不会报错，所以我们这边就是采用这样的思路，不过是本地版的代理服务器。


开始搭建

在官网下载：https://portswigger.net/burp/

我这边采用的抓包软件是burpsuit，这是一款非常牛逼的抓包软件，因为之前做渗透工作，所以burpsuit比较顺手。burpsuit的其他用法，我们可以百度一下，burpsuit这个比较出名的，我们下载这个抓包软件，然后设置他的证书（burpsuit抓https大家自行百度），让他可以支持https抓包就可以了，burpsuit安装前请安装java的环境，他是用java的开发，下载下来后，我们启动软件，点击  proxy  这个选项


burpsuit界面

然后我们点击 intercept is on 让它变成 intercept is off 状态，然后在点击 options，

burpsuit进入options

进入options，我们可以看见，我们现在的代理服务器是127.0.0.1:8080

代理界面

burpsuit一打开默认就是127.0.0.1:8080，这样就是一个简单的代理服务器。那么我们在爬虫中只要添加这个代理ip，就可以绕过ssl错误，当然你可以在浏览器internet设置成全局代理ip，这样就不用修改代码他也是成功的，我这边修改了代理，大家可以看看是否成功：


修改代码

给爬虫添加代理


验证方案

修改完爬虫，我们点击运行，效果是预想的一样的的，完美解决了https产生的ssl认证的问题

正常返回源码:


后话

总的来说，这种方法可以搞定一切https的问题，但是有一个问题，一直困扰我好久，代理ip的问题，我在本地设置代理服务器，代理的出口就是本地的ip，那么一旦我访问频率过高，我的代理服务器的ip也是会被封杀，所以这里呢需要一个代理服务器的二次代理的问题，貌似这个问题不太好解决；希望大家可以给一点思路解决这个问题，

有什么问题与建议可以联系我

如果你感觉文章可以，尽情点赞！！！收藏！！！您的点赞是我前进的动力！！！

