在安装过程中，请务必注意版本，本人在第一次安装过程中，python版本为3.8，spark版本为3.1.1的，故安装后，在运行pyspark的“动作”语句时，一直报错 Python worker failed to connect  back尝试很多办法都无法是解决这个问题，

最后只能将spark版本由3.1.1改为2.4.5，（即安装文件由spark-3.1.1-bin-hadoop2.7.tgz改为spark-2.4.5-bin-hadoop2.7.gz）

以上只是把spark的版本降低了，但是如果python版本不降低，仍然会报错如下，


TypeError:an integer is required（got type bytes）

因为 spark还不支持python3.8，所以需要将python版本将到3.7以下，本次我用了python3.6.6，完美解决

