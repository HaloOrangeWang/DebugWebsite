最近博主因为学习《云计算导论》一课，需要在Windows上搭建Saprk，故在网上找了相关教程搭建，步骤如下：

1. Scala

2.Spark

3.Handoop

但是不管博主怎么修正，在命令行输入“spark-shell”时，都会出现错误：

Missing Python executable 'python', defaulting to '............

对此博主找到了解决方法，如果你的报错也是这样，那么就按如下解决便可：

环境变量：

新建SPARK_HOME变量，变量值为：F:\spark-2.3.0-bin-hadoop2.7

路径和变量值，根据自己本机Spark的安装路径修改便可。

